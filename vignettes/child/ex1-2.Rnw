


We first demonstrate how the package can be used for curve fitting. We generate $y\sim N(f(x),1)$ where $x\in [-5,5]$ and
\begin{align}
f(x) &= 
\begin{cases} 
      -0.1 x^3 + 2\sin(\pi x^2)(x-4)^2 & 0<x<4 \\
      -0.1 x^3 & \text{otherwise}
   \end{cases}
\end{align}
for 1000 samples of $x$.  The data are shown in Figure \ref{fig:ex1plot3}.  

We generate the data with the following code.
<<c1-1,cache=T>>=
set.seed(0)
f <- function(x) {
  -.1 * x^3 + 2 * as.numeric((x < 4) * (x > 0)) * sin(pi * x^2) * (x - 4)^2
}
sigma <- 1
n <- 1000
x <- runif(n, -5, 5)
y <- rnorm(n, f(x), sigma)
@

We then call the \code{bass} function to fit a BASS model using the default settings.
<<c1-2,cache=T,dependson='c1-1'>>=
mod<-bass(x, y)
@
The result is an object that can be used for prediction and sensitivity analysis.  By default, the bass function prints progress after each 1000 MCMC iterations, along with the number of basis functions.  To diagnose the fit of the model, we call the plot function.
<<ex1plot1, fig.height=12*.5, fig.width=15*.5, out.width='.75\\linewidth', fig.cap='Diagnostic plots for BASS model fitting.'>>=
plot(mod)
@
This generates the four plots shown in Figure \ref{fig:ex1plot1}.  The top left and right plots show trace plots (after burn-in and excluding thinned samples) of the number of basis functions ($M$) and the error variance ($\sigma^2$).  The bottom left plot shows the response values plotted against the the posterior mean predictions (with equal tail posterior probability intervals as specified by the \code{quants} parameter).  The bottom right plot shows a histogram of the posterior mean residuals along with the assumed Gaussian distribution centered at zero and with variance taken to be the posterior mean of $\sigma^2$.  This is for checking the Normality assumption.



Next, we can generate posterior predictions at new inputs, which we generate as \code{x.test}.
<<c1-3,cache=T,dependson=c('c1-1','c1-2')>>=
n.test <- 1000
x.test <- sort(runif(n.test, -5, 5))
pred <- predict(mod, x.test, verbose = T)
@
By default, the \code{predict} function generates posterior predictive distributions for all of the inputs.  We can use a subset of posterior samples by specifying the parameter \code{mcmc.use}.  For instance, \code{mcmc.use = 1} will use the first posterior sample (after burn-in and excluding thinned samples), and will thus be faster.  Rather than iterating through the MCMC samples to generate predictions, we instead iterate through ``models."  The model changes when the basis functions change, which means that we can build the basis functions once and perform vectorized operations for predictions for all the MCMC iterations with the same basis functions.   

The object resulting from the \code{predict} function is a matrix with rows corresponding to MCMC samples and columns corresponding to settings of \code{x.test}.  Thus, the posterior mean predictions are obtained by taking the column means.  We plot the posterior predictive means against the true values of $f(x)$ as shown in Figure \ref{fig:ex1plot2}.
<<ex1plot2, fig.cap='BASS prediction on test data.',fig.width=6*.7,fig.height=6*.7>>=
fx.test <- f(x.test)
plot(fx.test, colMeans(pred))
abline(a = 0, b = 1, col = 2)
@

Note that the predictive distributions in the columns of \code{pred} are for $f(x)$.  To obtain predictive distributions for data, we would need to include Gaussian error with variance $\sigma^2$ (demonstrated in Section \ref{sec:ex5}).  Posterior samples of $\sigma^2$ are given in \code{mod$s2}.

In the curve fitting case, we can plot predicted curves.  Below, we plot 10 posterior predictive samples along with the true curve (Figure \ref{fig:ex1plot3}).  We also show knot locations (in the rug along the x-axis) for one of the posterior samples.
<<ex1plot3, fig.height=6*.8, fig.width=10*.8, out.width='.8\\linewidth', fig.cap='True curve with posterior predictive draws.',dev='pdf'>>=
plot(x, y, cex = .5)
curve(f(x), add = T, lwd = 3, n = 1000, col = 2, lty = 2)
matplot(x.test, t(pred[seq(100, 1000, 100), ]), type='l', add=T, col=3)
rug(BASS:::unscale.range(mod$curr.list[[1]]$knots.des, range(x)))
legend('topright', legend = c('true curve', 'posterior predictive draws'), 
       col = c(2:3), lty = c(2, 1), lwd = c(3, 1), bty = 'n')
@


If we are interested in using fewer knots (fewer basis functions), we can change the prior for the number of basis functions to be more restrictive.  For instance, setting \code{h2=100}
<<c1-4, cache=T, dependson='c1-1', results='hide'>>=
mod <- bass(x, y, h2 = 100)
@
<<ex1plot4, fig.height=6*.8, fig.width=10*.8, out.width='.8\\linewidth', fig.cap='True curve with posterior predictive draws and more restrictive prior on the number of basis functions.'>>=
pred <- predict(mod, x.test)
plot(x, y, cex = .5)
curve(f(x), add = T, lwd = 3, n = 1000, col = 2, lty = 2)
matplot(x.test, t(pred[seq(100, 1000, 100), ]), type='l', add=T, col=3)
rug(BASS:::unscale.range(mod$curr.list[[1]]$knots.des, range(x)))
legend('topright', legend = c('true curve', 'posterior predictive draws'), 
       col = c(2:3), lty = c(2, 1), lwd = c(3, 1), bty = 'n')
@
results in knots as shown along the x-axis of Figure \ref{fig:ex1plot4}.  This results in fewer knots, but perhaps slight underfitting in the part of the curve around $x = 3$.  The \code{h2} parameter can be used to prevent overfitting, but the setting is not intuitive.  Thus, this parameter may require tuning (perhaps by cross-validation).


Two final issues to discuss with this example are why we use linear splines (the default \code{degree = 1}) and how to tell if we have achieved convergence before taking MCMC samples as posterior samples.  We use linear splines almost exclusively when using this package because of their stability and ability to capture nonlinear curves and surfaces.  Using a higher degree, such as \code{degree = 3}, results in smoother models but suffers from stability problems and is more difficult to fit.  We suggest settings of \code{degree} other than \code{degree = 1} be used with care, always with scrutiny of prediction performance.  Convergence is best assessed by examining the trace plots shown in Figure \ref{fig:ex1plot1}.  Especially if the trace plot for $\sigma^2$ shows any sort of non-cyclical pattern, the sampler should be run for longer.  As a side note, a new sampler can be started from where the old sampler left off by using the \code{curr.list} parameter.  For instance, we can run \code{mod2 <- bass(x, y, curr.list = mod$curr.list)} to start a new sampler from where \code{mod} left off.
